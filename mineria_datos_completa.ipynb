{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MINER√çA DE DATOS COMPLETA - AN√ÅLISIS DE ART√çCULOS PERIOD√çSTICOS\n",
        "\n",
        "Este notebook implementa todos los algoritmos de miner√≠a de datos para analizar art√≠culos period√≠sticos y determinar cu√°l es el mejor modelo para clasificaci√≥n y clustering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. IMPORTACI√ìN DE LIBRER√çAS Y CARGA DE DATOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "df = pd.read_csv('articulos_exportados_20250926_082756.csv', sep=';')\n",
        "print(f\"Dataset cargado: {df.shape[0]} filas y {df.shape[1]} columnas\")\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis exploratorio de datos\n",
        "print(\"Informaci√≥n del dataset:\")\n",
        "print(df.info())\n",
        "print(\"\\nValores nulos:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nDistribuci√≥n de categor√≠as:\")\n",
        "print(df['Categor√≠a'].value_counts())\n",
        "print(\"\\nDistribuci√≥n de peri√≥dicos:\")\n",
        "print(df['Peri√≥dico'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PREPARACI√ìN DE DATOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpieza y preparaci√≥n de datos\n",
        "# Eliminar filas con valores nulos en columnas importantes\n",
        "df_clean = df.dropna(subset=['T√≠tulo', 'Categor√≠a', 'Peri√≥dico'])\n",
        "\n",
        "# Crear variable objetivo binaria basada en categor√≠a\n",
        "# Clasificaremos como 'Noticias importantes' vs 'Otras'\n",
        "categorias_importantes = ['Internacional', 'Pol√≠tica', 'Econom√≠a']\n",
        "df_clean['es_importante'] = df_clean['Categor√≠a'].isin(categorias_importantes).astype(int)\n",
        "\n",
        "# Combinar t√≠tulo y resumen para an√°lisis de texto\n",
        "df_clean['texto_completo'] = df_clean['T√≠tulo'].fillna('') + ' ' + df_clean['Resumen'].fillna('')\n",
        "\n",
        "# Crear features num√©ricas\n",
        "df_clean['longitud_titulo'] = df_clean['T√≠tulo'].str.len()\n",
        "df_clean['longitud_resumen'] = df_clean['Resumen'].str.len()\n",
        "df_clean['cantidad_imagenes'] = df_clean['Cantidad Im√°genes'].fillna(0)\n",
        "\n",
        "# Codificar variables categ√≥ricas\n",
        "le_periodico = LabelEncoder()\n",
        "df_clean['periodico_encoded'] = le_periodico.fit_transform(df_clean['Peri√≥dico'])\n",
        "\n",
        "print(f\"Dataset limpio: {df_clean.shape[0]} filas\")\n",
        "print(f\"Distribuci√≥n de clases: {df_clean['es_importante'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorizaci√≥n de texto usando TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=1000, stop_words='spanish', ngram_range=(1, 2))\n",
        "X_texto = tfidf.fit_transform(df_clean['texto_completo'])\n",
        "\n",
        "# Features num√©ricas\n",
        "X_numericas = df_clean[['longitud_titulo', 'longitud_resumen', 'cantidad_imagenes', 'periodico_encoded']].values\n",
        "\n",
        "# Combinar features\n",
        "from scipy.sparse import hstack\n",
        "X = hstack([X_texto, X_numericas])\n",
        "y = df_clean['es_importante'].values\n",
        "\n",
        "# Divisi√≥n en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Conjunto de prueba: {X_test.shape}\")\n",
        "print(f\"Distribuci√≥n de clases en entrenamiento: {np.bincount(y_train)}\")\n",
        "print(f\"Distribuci√≥n de clases en prueba: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. FUNCI√ìN DE EVALUACI√ìN DE MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mide_error(nombre_modelo, y_pred_proba, y_true=y_test):\n",
        "    \"\"\"\n",
        "    Funci√≥n para medir el error y rendimiento de los modelos\n",
        "    \"\"\"\n",
        "    # Convertir probabilidades a predicciones binarias\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"\\n=== {nombre_modelo} ===\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "    print(\"\\nReporte de clasificaci√≥n:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    \n",
        "    return {\n",
        "        'modelo': nombre_modelo,\n",
        "        'accuracy': accuracy,\n",
        "        'auc': auc,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "print(\"Funci√≥n de evaluaci√≥n creada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ALGORITMOS DE CLASIFICACI√ìN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 REGRESI√ìN LOG√çSTICA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# REGRESI√ìN LOG√çSTICA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(solver='newton-cg', random_state=42, max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_logreg = logreg.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_logreg = mide_error('Regresi√≥n Log√≠stica', y_pred_logreg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 K-NEAREST NEIGHBORS (KNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_knn = mide_error('KNN', y_pred_knn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 NAIVE BAYES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NAIVE BAYES\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "naive_bayes = BernoulliNB()\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "y_pred_nb = naive_bayes.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_nb = mide_error('Naive Bayes', y_pred_nb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 √ÅRBOL DE DECISI√ìN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# √ÅRBOL DE DECISI√ìN\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_tree = mide_error('√Årbol de Decisi√≥n', y_pred_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 RANDOM FOREST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RANDOM FOREST\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier(random_state=42)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_rf = mide_error('Random Forest', y_pred_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 XGBOOST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBOOST\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    \n",
        "    xgb_classifier = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_classifier.predict_proba(X_test)[:,1]\n",
        "    \n",
        "    resultados_xgb = mide_error('XGBoost', y_pred_xgb)\n",
        "except ImportError:\n",
        "    print(\"XGBoost no est√° instalado. Instalando...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
        "    \n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_classifier = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_classifier.predict_proba(X_test)[:,1]\n",
        "    \n",
        "    resultados_xgb = mide_error('XGBoost', y_pred_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.7 LIGHTGBM (HISTOGRADIENTBOOSTING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LIGHTGBM (usando HistGradientBoostingClassifier de sklearn)\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hist_gradient_boosting = HistGradientBoostingClassifier(random_state=42)\n",
        "hist_gradient_boosting.fit(X_train, y_train)\n",
        "y_pred_lgb = hist_gradient_boosting.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_lgb = mide_error('LightGBM (HistGradientBoosting)', y_pred_lgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ALGORITMOS DE CLUSTERING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 K-MEDIAS (K-MEANS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-MEDIAS\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Reducir dimensionalidad para clustering\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_pca = pca.fit_transform(X_train.toarray())\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "kmeans.fit(X_pca)\n",
        "\n",
        "print(\"\\n=== K-MEDIAS ===\")\n",
        "print(f\"Centroides encontrados: {kmeans.n_clusters}\")\n",
        "print(f\"Primeros 10 labels de cluster: {kmeans.labels_[:10]}\")\n",
        "print(f\"Inercia: {kmeans.inertia_:.2f}\")\n",
        "\n",
        "# Visualizaci√≥n de clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.6)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
        "           c='red', marker='x', s=200, linewidths=3)\n",
        "plt.title('Clustering K-Means de Art√≠culos Period√≠sticos')\n",
        "plt.xlabel('Primera Componente Principal')\n",
        "plt.ylabel('Segunda Componente Principal')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. AN√ÅLISIS DE SERIES TEMPORALES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 PREPARACI√ìN DE DATOS TEMPORALES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para an√°lisis temporal\n",
        "df_clean['Fecha Extracci√≥n'] = pd.to_datetime(df_clean['Fecha Extracci√≥n'])\n",
        "df_clean['fecha'] = df_clean['Fecha Extracci√≥n'].dt.date\n",
        "\n",
        "# Agrupar por fecha y contar art√≠culos importantes\n",
        "serie_temporal = df_clean.groupby('fecha')['es_importante'].agg(['count', 'sum']).reset_index()\n",
        "serie_temporal.columns = ['fecha', 'total_articulos', 'articulos_importantes']\n",
        "serie_temporal['proporcion_importantes'] = serie_temporal['articulos_importantes'] / serie_temporal['total_articulos']\n",
        "serie_temporal = serie_temporal.sort_values('fecha')\n",
        "\n",
        "print(\"Serie temporal creada:\")\n",
        "print(serie_temporal.head())\n",
        "print(f\"\\nTotal de d√≠as: {len(serie_temporal)}\")\n",
        "\n",
        "# Visualizaci√≥n de la serie temporal\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(serie_temporal['fecha'], serie_temporal['proporcion_importantes'], marker='o')\n",
        "plt.title('Proporci√≥n de Art√≠culos Importantes a lo Largo del Tiempo')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Proporci√≥n de Art√≠culos Importantes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 ARIMA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA\n",
        "try:\n",
        "    from pmdarima import auto_arima\n",
        "    \n",
        "    # Preparar datos para ARIMA\n",
        "    train_data = serie_temporal['proporcion_importantes'].values[:-4]  # √öltimos 4 puntos para test\n",
        "    test_data = serie_temporal['proporcion_importantes'].values[-4:]\n",
        "    \n",
        "    model = auto_arima(train_data, seasonal=False, suppress_warnings=True)\n",
        "    model_fit = model.fit(train_data)\n",
        "    predictions = model_fit.predict(n_periods=len(test_data))\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(serie_temporal['fecha'][-12:].astype(str), serie_temporal['proporcion_importantes'][-12:], label='Realidad')\n",
        "    plt.plot(serie_temporal['fecha'][-4:].astype(str), predictions, label='Predicci√≥n')\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "    plt.title('Predicci√≥n ARIMA - Proporci√≥n de Art√≠culos Importantes')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== ARIMA ===\")\n",
        "    print(f\"Predicciones: {predictions}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"pmdarima no est√° instalado. Instalando...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'pmdarima'])\n",
        "    \n",
        "    from pmdarima import auto_arima\n",
        "    train_data = serie_temporal['proporcion_importantes'].values[:-4]\n",
        "    test_data = serie_temporal['proporcion_importantes'].values[-4:]\n",
        "    \n",
        "    model = auto_arima(train_data, seasonal=False, suppress_warnings=True)\n",
        "    model_fit = model.fit(train_data)\n",
        "    predictions = model_fit.predict(n_periods=len(test_data))\n",
        "    \n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(serie_temporal['fecha'][-12:].astype(str), serie_temporal['proporcion_importantes'][-12:], label='Realidad')\n",
        "    plt.plot(serie_temporal['fecha'][-4:].astype(str), predictions, label='Predicci√≥n')\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "    plt.title('Predicci√≥n ARIMA - Proporci√≥n de Art√≠culos Importantes')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 SUAVIZADO EXPONENCIAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SUAVIZADO EXPONENCIAL\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "train_data = serie_temporal['proporcion_importantes'].values[:-4]\n",
        "test_data = serie_temporal['proporcion_importantes'].values[-4:]\n",
        "\n",
        "model = ExponentialSmoothing(train_data, seasonal=None, trend='add')\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(serie_temporal['fecha'][-12:].astype(str), serie_temporal['proporcion_importantes'][-12:], label='Realidad')\n",
        "plt.plot(serie_temporal['fecha'][-4:].astype(str), predictions, label='Predicci√≥n')\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "plt.title('Suavizado Exponencial - Proporci√≥n de Art√≠culos Importantes')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== SUAVIZADO EXPONENCIAL ===\")\n",
        "print(f\"Predicciones: {predictions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. COMPARACI√ìN Y EVALUACI√ìN DE MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recopilar todos los resultados\n",
        "resultados = [\n",
        "    resultados_logreg,\n",
        "    resultados_knn,\n",
        "    resultados_nb,\n",
        "    resultados_tree,\n",
        "    resultados_rf,\n",
        "    resultados_xgb,\n",
        "    resultados_lgb\n",
        "]\n",
        "\n",
        "# Crear DataFrame de comparaci√≥n\n",
        "comparacion = pd.DataFrame([\n",
        "    {\n",
        "        'Modelo': r['modelo'],\n",
        "        'Accuracy': r['accuracy'],\n",
        "        'AUC-ROC': r['auc']\n",
        "    } for r in resultados\n",
        "])\n",
        "\n",
        "print(\"\\n=== COMPARACI√ìN DE MODELOS ===\")\n",
        "print(comparacion.sort_values('AUC-ROC', ascending=False))\n",
        "\n",
        "# Visualizaci√≥n de comparaci√≥n\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gr√°fico de barras para Accuracy\n",
        "ax1.bar(comparacion['Modelo'], comparacion['Accuracy'], color='skyblue', alpha=0.7)\n",
        "ax1.set_title('Comparaci√≥n de Accuracy')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gr√°fico de barras para AUC-ROC\n",
        "ax2.bar(comparacion['Modelo'], comparacion['AUC-ROC'], color='lightcoral', alpha=0.7)\n",
        "ax2.set_title('Comparaci√≥n de AUC-ROC')\n",
        "ax2.set_ylabel('AUC-ROC')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mejor modelo\n",
        "mejor_modelo = comparacion.loc[comparacion['AUC-ROC'].idxmax()]\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {mejor_modelo['Modelo']}\")\n",
        "print(f\"   Accuracy: {mejor_modelo['Accuracy']:.4f}\")\n",
        "print(f\"   AUC-ROC: {mejor_modelo['AUC-ROC']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de importancia de caracter√≠sticas (usando Random Forest)\n",
        "feature_names = list(tfidf.get_feature_names_out()) + ['longitud_titulo', 'longitud_resumen', 'cantidad_imagenes', 'periodico_encoded']\n",
        "\n",
        "importancias = random_forest.feature_importances_\n",
        "indices = np.argsort(importancias)[::-1][:20]  # Top 20 caracter√≠sticas\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top 20 Caracter√≠sticas M√°s Importantes')\n",
        "plt.barh(range(len(indices)), importancias[indices], color='lightgreen', alpha=0.7)\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Importancia')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 caracter√≠sticas m√°s importantes:\")\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}. {feature_names[indices[i]]}: {importancias[indices[i]]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. CONCLUSIONES Y RECOMENDACIONES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"           RESUMEN DE AN√ÅLISIS DE MINER√çA DE DATOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä DATASET ANALIZADO:\")\n",
        "print(f\"   ‚Ä¢ Total de art√≠culos: {len(df_clean)}\")\n",
        "print(f\"   ‚Ä¢ Peri√≥dicos: {df_clean['Peri√≥dico'].nunique()}\")\n",
        "print(f\"   ‚Ä¢ Categor√≠as: {df_clean['Categor√≠a'].nunique()}\")\n",
        "print(f\"   ‚Ä¢ Proporci√≥n de art√≠culos importantes: {df_clean['es_importante'].mean():.2%}\")\n",
        "\n",
        "print(f\"\\nü§ñ MODELOS EVALUADOS:\")\n",
        "for i, (_, row) in enumerate(comparacion.sort_values('AUC-ROC', ascending=False).iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Modelo']}: AUC={row['AUC-ROC']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {mejor_modelo['Modelo']}\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {mejor_modelo['Accuracy']:.4f}\")\n",
        "print(f\"   ‚Ä¢ AUC-ROC: {mejor_modelo['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà INSIGHTS CLAVE:\")\n",
        "print(f\"   ‚Ä¢ El modelo {mejor_modelo['Modelo']} es el m√°s efectivo para clasificar art√≠culos\")\n",
        "print(f\"   ‚Ä¢ Las caracter√≠sticas de texto son las m√°s importantes\")\n",
        "print(f\"   ‚Ä¢ El clustering revela patrones en la estructura de los art√≠culos\")\n",
        "print(f\"   ‚Ä¢ Las series temporales muestran tendencias en la importancia de noticias\")\n",
        "\n",
        "print(f\"\\nüí° RECOMENDACIONES:\")\n",
        "print(f\"   ‚Ä¢ Usar {mejor_modelo['Modelo']} para clasificaci√≥n autom√°tica\")\n",
        "print(f\"   ‚Ä¢ Implementar an√°lisis de sentimiento en el texto\")\n",
        "print(f\"   ‚Ä¢ Monitorear tendencias temporales de importancia\")\n",
        "print(f\"   ‚Ä¢ Considerar features adicionales como hora de publicaci√≥n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
