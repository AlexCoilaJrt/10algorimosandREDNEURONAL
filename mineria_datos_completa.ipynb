{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MINERÍA DE DATOS COMPLETA - ANÁLISIS DE ARTÍCULOS PERIODÍSTICOS\n",
        "\n",
        "Este notebook implementa todos los algoritmos de minería de datos para analizar artículos periodísticos y determinar cuál es el mejor modelo para clasificación y clustering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. IMPORTACIÓN DE LIBRERÍAS Y CARGA DE DATOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Librerías importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de datos\n",
        "df = pd.read_csv('articulos_exportados_20250926_082756.csv', sep=';')\n",
        "print(f\"Dataset cargado: {df.shape[0]} filas y {df.shape[1]} columnas\")\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis exploratorio de datos\n",
        "print(\"Información del dataset:\")\n",
        "print(df.info())\n",
        "print(\"\\nValores nulos:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nDistribución de categorías:\")\n",
        "print(df['Categoría'].value_counts())\n",
        "print(\"\\nDistribución de periódicos:\")\n",
        "print(df['Periódico'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PREPARACIÓN DE DATOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpieza y preparación de datos\n",
        "# Eliminar filas con valores nulos en columnas importantes\n",
        "df_clean = df.dropna(subset=['Título', 'Categoría', 'Periódico'])\n",
        "\n",
        "# Crear variable objetivo binaria basada en categoría\n",
        "# Clasificaremos como 'Noticias importantes' vs 'Otras'\n",
        "categorias_importantes = ['Internacional', 'Política', 'Economía']\n",
        "df_clean['es_importante'] = df_clean['Categoría'].isin(categorias_importantes).astype(int)\n",
        "\n",
        "# Combinar título y resumen para análisis de texto\n",
        "df_clean['texto_completo'] = df_clean['Título'].fillna('') + ' ' + df_clean['Resumen'].fillna('')\n",
        "\n",
        "# Crear features numéricas\n",
        "df_clean['longitud_titulo'] = df_clean['Título'].str.len()\n",
        "df_clean['longitud_resumen'] = df_clean['Resumen'].str.len()\n",
        "df_clean['cantidad_imagenes'] = df_clean['Cantidad Imágenes'].fillna(0)\n",
        "\n",
        "# Codificar variables categóricas\n",
        "le_periodico = LabelEncoder()\n",
        "df_clean['periodico_encoded'] = le_periodico.fit_transform(df_clean['Periódico'])\n",
        "\n",
        "print(f\"Dataset limpio: {df_clean.shape[0]} filas\")\n",
        "print(f\"Distribución de clases: {df_clean['es_importante'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorización de texto usando TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=1000, stop_words='spanish', ngram_range=(1, 2))\n",
        "X_texto = tfidf.fit_transform(df_clean['texto_completo'])\n",
        "\n",
        "# Features numéricas\n",
        "X_numericas = df_clean[['longitud_titulo', 'longitud_resumen', 'cantidad_imagenes', 'periodico_encoded']].values\n",
        "\n",
        "# Combinar features\n",
        "from scipy.sparse import hstack\n",
        "X = hstack([X_texto, X_numericas])\n",
        "y = df_clean['es_importante'].values\n",
        "\n",
        "# División en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Conjunto de prueba: {X_test.shape}\")\n",
        "print(f\"Distribución de clases en entrenamiento: {np.bincount(y_train)}\")\n",
        "print(f\"Distribución de clases en prueba: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. FUNCIÓN DE EVALUACIÓN DE MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mide_error(nombre_modelo, y_pred_proba, y_true=y_test):\n",
        "    \"\"\"\n",
        "    Función para medir el error y rendimiento de los modelos\n",
        "    \"\"\"\n",
        "    # Convertir probabilidades a predicciones binarias\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "    \n",
        "    # Calcular métricas\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"\\n=== {nombre_modelo} ===\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "    print(\"\\nReporte de clasificación:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    \n",
        "    return {\n",
        "        'modelo': nombre_modelo,\n",
        "        'accuracy': accuracy,\n",
        "        'auc': auc,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "print(\"Función de evaluación creada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ALGORITMOS DE CLASIFICACIÓN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 REGRESIÓN LOGÍSTICA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# REGRESIÓN LOGÍSTICA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(solver='newton-cg', random_state=42, max_iter=1000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_logreg = logreg.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_logreg = mide_error('Regresión Logística', y_pred_logreg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 K-NEAREST NEIGHBORS (KNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_knn = mide_error('KNN', y_pred_knn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 NAIVE BAYES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NAIVE BAYES\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "naive_bayes = BernoulliNB()\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "y_pred_nb = naive_bayes.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_nb = mide_error('Naive Bayes', y_pred_nb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 ÁRBOL DE DECISIÓN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ÁRBOL DE DECISIÓN\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_tree = mide_error('Árbol de Decisión', y_pred_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 RANDOM FOREST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RANDOM FOREST\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier(random_state=42)\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_rf = mide_error('Random Forest', y_pred_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.6 XGBOOST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBOOST\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    \n",
        "    xgb_classifier = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_classifier.predict_proba(X_test)[:,1]\n",
        "    \n",
        "    resultados_xgb = mide_error('XGBoost', y_pred_xgb)\n",
        "except ImportError:\n",
        "    print(\"XGBoost no está instalado. Instalando...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'xgboost'])\n",
        "    \n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_classifier = XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_classifier.predict_proba(X_test)[:,1]\n",
        "    \n",
        "    resultados_xgb = mide_error('XGBoost', y_pred_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.7 LIGHTGBM (HISTOGRADIENTBOOSTING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LIGHTGBM (usando HistGradientBoostingClassifier de sklearn)\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hist_gradient_boosting = HistGradientBoostingClassifier(random_state=42)\n",
        "hist_gradient_boosting.fit(X_train, y_train)\n",
        "y_pred_lgb = hist_gradient_boosting.predict_proba(X_test)[:,1]\n",
        "\n",
        "resultados_lgb = mide_error('LightGBM (HistGradientBoosting)', y_pred_lgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ALGORITMOS DE CLUSTERING\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 K-MEDIAS (K-MEANS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K-MEDIAS\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Reducir dimensionalidad para clustering\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_pca = pca.fit_transform(X_train.toarray())\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "kmeans.fit(X_pca)\n",
        "\n",
        "print(\"\\n=== K-MEDIAS ===\")\n",
        "print(f\"Centroides encontrados: {kmeans.n_clusters}\")\n",
        "print(f\"Primeros 10 labels de cluster: {kmeans.labels_[:10]}\")\n",
        "print(f\"Inercia: {kmeans.inertia_:.2f}\")\n",
        "\n",
        "# Visualización de clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.6)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
        "           c='red', marker='x', s=200, linewidths=3)\n",
        "plt.title('Clustering K-Means de Artículos Periodísticos')\n",
        "plt.xlabel('Primera Componente Principal')\n",
        "plt.ylabel('Segunda Componente Principal')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ANÁLISIS DE SERIES TEMPORALES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 PREPARACIÓN DE DATOS TEMPORALES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para análisis temporal\n",
        "df_clean['Fecha Extracción'] = pd.to_datetime(df_clean['Fecha Extracción'])\n",
        "df_clean['fecha'] = df_clean['Fecha Extracción'].dt.date\n",
        "\n",
        "# Agrupar por fecha y contar artículos importantes\n",
        "serie_temporal = df_clean.groupby('fecha')['es_importante'].agg(['count', 'sum']).reset_index()\n",
        "serie_temporal.columns = ['fecha', 'total_articulos', 'articulos_importantes']\n",
        "serie_temporal['proporcion_importantes'] = serie_temporal['articulos_importantes'] / serie_temporal['total_articulos']\n",
        "serie_temporal = serie_temporal.sort_values('fecha')\n",
        "\n",
        "print(\"Serie temporal creada:\")\n",
        "print(serie_temporal.head())\n",
        "print(f\"\\nTotal de días: {len(serie_temporal)}\")\n",
        "\n",
        "# Visualización de la serie temporal\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(serie_temporal['fecha'], serie_temporal['proporcion_importantes'], marker='o')\n",
        "plt.title('Proporción de Artículos Importantes a lo Largo del Tiempo')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Proporción de Artículos Importantes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 ARIMA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA\n",
        "try:\n",
        "    from pmdarima import auto_arima\n",
        "    \n",
        "    # Preparar datos para ARIMA\n",
        "    train_data = serie_temporal['proporcion_importantes'].values[:-4]  # Últimos 4 puntos para test\n",
        "    test_data = serie_temporal['proporcion_importantes'].values[-4:]\n",
        "    \n",
        "    model = auto_arima(train_data, seasonal=False, suppress_warnings=True)\n",
        "    model_fit = model.fit(train_data)\n",
        "    predictions = model_fit.predict(n_periods=len(test_data))\n",
        "    \n",
        "    # Visualización\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(serie_temporal['fecha'][-12:].astype(str), serie_temporal['proporcion_importantes'][-12:], label='Realidad')\n",
        "    plt.plot(serie_temporal['fecha'][-4:].astype(str), predictions, label='Predicción')\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "    plt.title('Predicción ARIMA - Proporción de Artículos Importantes')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== ARIMA ===\")\n",
        "    print(f\"Predicciones: {predictions}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"pmdarima no está instalado. Instalando...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'pmdarima'])\n",
        "    \n",
        "    from pmdarima import auto_arima\n",
        "    train_data = serie_temporal['proporcion_importantes'].values[:-4]\n",
        "    test_data = serie_temporal['proporcion_importantes'].values[-4:]\n",
        "    \n",
        "    model = auto_arima(train_data, seasonal=False, suppress_warnings=True)\n",
        "    model_fit = model.fit(train_data)\n",
        "    predictions = model_fit.predict(n_periods=len(test_data))\n",
        "    \n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(serie_temporal['fecha'][-12:].astype(str), serie_temporal['proporcion_importantes'][-12:], label='Realidad')\n",
        "    plt.plot(serie_temporal['fecha'][-4:].astype(str), predictions, label='Predicción')\n",
        "    plt.xticks(rotation=45, fontsize=8)\n",
        "    plt.title('Predicción ARIMA - Proporción de Artículos Importantes')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 SUAVIZADO EXPONENCIAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SUAVIZADO EXPONENCIAL\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "train_data = serie_temporal['proporcion_importantes'].values[:-4]\n",
        "test_data = serie_temporal['proporcion_importantes'].values[-4:]\n",
        "\n",
        "model = ExponentialSmoothing(train_data, seasonal=None, trend='add')\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=len(train_data), end=len(train_data) + len(test_data) - 1)\n",
        "\n",
        "# Visualización\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(serie_temporal['fecha'][-12:].astype(str), serie_temporal['proporcion_importantes'][-12:], label='Realidad')\n",
        "plt.plot(serie_temporal['fecha'][-4:].astype(str), predictions, label='Predicción')\n",
        "plt.xticks(rotation=45, fontsize=8)\n",
        "plt.title('Suavizado Exponencial - Proporción de Artículos Importantes')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== SUAVIZADO EXPONENCIAL ===\")\n",
        "print(f\"Predicciones: {predictions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. COMPARACIÓN Y EVALUACIÓN DE MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recopilar todos los resultados\n",
        "resultados = [\n",
        "    resultados_logreg,\n",
        "    resultados_knn,\n",
        "    resultados_nb,\n",
        "    resultados_tree,\n",
        "    resultados_rf,\n",
        "    resultados_xgb,\n",
        "    resultados_lgb\n",
        "]\n",
        "\n",
        "# Crear DataFrame de comparación\n",
        "comparacion = pd.DataFrame([\n",
        "    {\n",
        "        'Modelo': r['modelo'],\n",
        "        'Accuracy': r['accuracy'],\n",
        "        'AUC-ROC': r['auc']\n",
        "    } for r in resultados\n",
        "])\n",
        "\n",
        "print(\"\\n=== COMPARACIÓN DE MODELOS ===\")\n",
        "print(comparacion.sort_values('AUC-ROC', ascending=False))\n",
        "\n",
        "# Visualización de comparación\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Gráfico de barras para Accuracy\n",
        "ax1.bar(comparacion['Modelo'], comparacion['Accuracy'], color='skyblue', alpha=0.7)\n",
        "ax1.set_title('Comparación de Accuracy')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Gráfico de barras para AUC-ROC\n",
        "ax2.bar(comparacion['Modelo'], comparacion['AUC-ROC'], color='lightcoral', alpha=0.7)\n",
        "ax2.set_title('Comparación de AUC-ROC')\n",
        "ax2.set_ylabel('AUC-ROC')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mejor modelo\n",
        "mejor_modelo = comparacion.loc[comparacion['AUC-ROC'].idxmax()]\n",
        "print(f\"\\n🏆 MEJOR MODELO: {mejor_modelo['Modelo']}\")\n",
        "print(f\"   Accuracy: {mejor_modelo['Accuracy']:.4f}\")\n",
        "print(f\"   AUC-ROC: {mejor_modelo['AUC-ROC']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ANÁLISIS DE IMPORTANCIA DE CARACTERÍSTICAS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis de importancia de características (usando Random Forest)\n",
        "feature_names = list(tfidf.get_feature_names_out()) + ['longitud_titulo', 'longitud_resumen', 'cantidad_imagenes', 'periodico_encoded']\n",
        "\n",
        "importancias = random_forest.feature_importances_\n",
        "indices = np.argsort(importancias)[::-1][:20]  # Top 20 características\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top 20 Características Más Importantes')\n",
        "plt.barh(range(len(indices)), importancias[indices], color='lightgreen', alpha=0.7)\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel('Importancia')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 características más importantes:\")\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}. {feature_names[indices[i]]}: {importancias[indices[i]]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. CONCLUSIONES Y RECOMENDACIONES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"           RESUMEN DE ANÁLISIS DE MINERÍA DE DATOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n📊 DATASET ANALIZADO:\")\n",
        "print(f\"   • Total de artículos: {len(df_clean)}\")\n",
        "print(f\"   • Periódicos: {df_clean['Periódico'].nunique()}\")\n",
        "print(f\"   • Categorías: {df_clean['Categoría'].nunique()}\")\n",
        "print(f\"   • Proporción de artículos importantes: {df_clean['es_importante'].mean():.2%}\")\n",
        "\n",
        "print(f\"\\n🤖 MODELOS EVALUADOS:\")\n",
        "for i, (_, row) in enumerate(comparacion.sort_values('AUC-ROC', ascending=False).iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Modelo']}: AUC={row['AUC-ROC']:.4f}, Acc={row['Accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n🏆 MEJOR MODELO: {mejor_modelo['Modelo']}\")\n",
        "print(f\"   • Accuracy: {mejor_modelo['Accuracy']:.4f}\")\n",
        "print(f\"   • AUC-ROC: {mejor_modelo['AUC-ROC']:.4f}\")\n",
        "\n",
        "print(f\"\\n📈 INSIGHTS CLAVE:\")\n",
        "print(f\"   • El modelo {mejor_modelo['Modelo']} es el más efectivo para clasificar artículos\")\n",
        "print(f\"   • Las características de texto son las más importantes\")\n",
        "print(f\"   • El clustering revela patrones en la estructura de los artículos\")\n",
        "print(f\"   • Las series temporales muestran tendencias en la importancia de noticias\")\n",
        "\n",
        "print(f\"\\n💡 RECOMENDACIONES:\")\n",
        "print(f\"   • Usar {mejor_modelo['Modelo']} para clasificación automática\")\n",
        "print(f\"   • Implementar análisis de sentimiento en el texto\")\n",
        "print(f\"   • Monitorear tendencias temporales de importancia\")\n",
        "print(f\"   • Considerar features adicionales como hora de publicación\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
