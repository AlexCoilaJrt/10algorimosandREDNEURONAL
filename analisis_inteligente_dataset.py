#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
An√°lisis Inteligente del Dataset de Art√≠culos Period√≠sticos
Usando variables realmente √∫tiles y relevantes para el an√°lisis
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, silhouette_score
from scipy.sparse import hstack
import json
import warnings
from datetime import datetime
import re
warnings.filterwarnings('ignore')

def cargar_y_analizar_dataset():
    """Cargar y analizar el dataset para entender las variables √∫tiles"""
    print("üìä Cargando y analizando dataset...")
    
    # Cargar datos
    df = pd.read_csv('articulos_exportados_20250926_082756.csv', sep=';')
    print(f"‚úÖ Dataset cargado: {df.shape[0]} art√≠culos, {df.shape[1]} columnas")
    
    # An√°lisis exploratorio
    print(f"\nüîç AN√ÅLISIS EXPLORATORIO:")
    print(f"   ‚Ä¢ Peri√≥dicos √∫nicos: {df['Peri√≥dico'].nunique()}")
    print(f"   ‚Ä¢ Categor√≠as √∫nicas: {df['Categor√≠a'].nunique()}")
    print(f"   ‚Ä¢ Regiones √∫nicas: {df['Regi√≥n'].nunique()}")
    print(f"   ‚Ä¢ Art√≠culos con im√°genes: {df['Cantidad Im√°genes'].sum()}")
    
    # Limpiar datos
    df = df.dropna(subset=['T√≠tulo', 'Contenido'])
    df['T√≠tulo'] = df['T√≠tulo'].fillna('')
    df['Resumen'] = df['Resumen'].fillna('')
    df['Contenido'] = df['Contenido'].fillna('')
    
    print(f"‚úÖ Datos limpiados: {df.shape[0]} art√≠culos")
    return df

def crear_variables_inteligentes(df):
    """
    Crear variables realmente √∫tiles para an√°lisis period√≠stico
    """
    print("üß† Creando variables inteligentes...")
    
    # 1. AN√ÅLISIS DE CONTENIDO TEXTUAL
    df['longitud_titulo'] = df['T√≠tulo'].str.len()
    df['longitud_resumen'] = df['Resumen'].str.len()
    df['longitud_contenido'] = df['Contenido'].str.len()
    df['palabras_titulo'] = df['T√≠tulo'].str.split().str.len()
    df['palabras_contenido'] = df['Contenido'].str.split().str.len()
    
    # 2. AN√ÅLISIS DE COMPLEJIDAD DEL TEXTO
    def calcular_complejidad_texto(texto):
        if pd.isna(texto) or texto == '':
            return 0
        # Palabras por oraci√≥n (complejidad)
        oraciones = len(re.split(r'[.!?]+', str(texto)))
        palabras = len(str(texto).split())
        return palabras / max(oraciones, 1)
    
    df['complejidad_titulo'] = df['T√≠tulo'].apply(calcular_complejidad_texto)
    df['complejidad_contenido'] = df['Contenido'].apply(calcular_complejidad_texto)
    
    # 3. AN√ÅLISIS DE TEMAS IMPORTANTES (palabras clave period√≠sticas)
    palabras_importantes = {
        'politica': ['gobierno', 'pol√≠tica', 'elecciones', 'parlamento', 'ministro', 'presidente', 'congreso'],
        'economia': ['econom√≠a', 'financiero', 'mercado', 'empresa', 'negocio', 'inversi√≥n', 'crisis'],
        'internacional': ['internacional', 'mundo', 'global', 'pa√≠s', 'naci√≥n', 'extranjero', 'diplomacia'],
        'social': ['sociedad', 'social', 'comunidad', 'p√∫blico', 'ciudadano', 'derechos', 'justicia'],
        'tecnologia': ['tecnolog√≠a', 'digital', 'innovaci√≥n', 'ciencia', 'investigaci√≥n', 'desarrollo'],
        'cultura': ['cultura', 'arte', 'literatura', 'm√∫sica', 'cine', 'teatro', 'exposici√≥n']
    }
    
    for tema, palabras in palabras_importantes.items():
        def contar_palabras_tema(texto):
            if pd.isna(texto):
                return 0
            texto_lower = str(texto).lower()
            return sum(1 for palabra in palabras if palabra in texto_lower)
        
        df[f'conteo_{tema}'] = df['Contenido'].apply(contar_palabras_tema)
    
    # 4. AN√ÅLISIS DE ESTRUCTURA PERIOD√çSTICA
    # T√≠tulos que siguen formato period√≠stico (m√°s informativos)
    df['titulo_informativo'] = (
        (df['longitud_titulo'] > df['longitud_titulo'].quantile(0.6)) &
        (df['palabras_titulo'] >= 5) &
        (df['T√≠tulo'].str.contains(r'[A-Z][a-z]+.*[A-Z][a-z]+', regex=True))  # M√∫ltiples palabras capitalizadas
    )
    
    # Contenido con estructura period√≠stica
    df['contenido_estructurado'] = (
        (df['longitud_contenido'] > df['longitud_contenido'].quantile(0.7)) &
        (df['palabras_contenido'] >= 100) &
        (df['complejidad_contenido'] > df['complejidad_contenido'].quantile(0.5))
    )
    
    # 5. AN√ÅLISIS DE PERI√ìDICOS (prestigio period√≠stico real)
    # Basado en reconocimiento period√≠stico y calidad editorial
    periodicos_prestigio = {
        'alto': ['La Vanguardia', 'Elmundo', 'Nytimes'],
        'medio': ['El Comercio', 'Peru21', 'El Peruano'],
        'bajo': ['Trome', 'Ojo', 'El Popular']
    }
    
    def asignar_prestigio(periodico):
        for nivel, periodicos in periodicos_prestigio.items():
            if periodico in periodicos:
                return nivel
        return 'medio'  # Por defecto
    
    df['prestigio_periodico'] = df['Peri√≥dico'].apply(asignar_prestigio)
    
    # 6. AN√ÅLISIS DE CATEGOR√çAS (relevancia period√≠stica)
    categorias_relevantes = {
        'alta': ['Internacional', 'Pol√≠tica', 'Econom√≠a', 'Ciencia y Salud'],
        'media': ['Cultura', 'Sociedad', 'Actualidad', 'Mundo'],
        'baja': ['Deportes', 'Espect√°culos', 'Hor√≥scopo', 'Vida']
    }
    
    def asignar_relevancia_categoria(categoria):
        for nivel, categorias in categorias_relevantes.items():
            if categoria in categorias:
                return nivel
        return 'media'
    
    df['relevancia_categoria'] = df['Categor√≠a'].apply(asignar_relevancia_categoria)
    
    # 7. AN√ÅLISIS TEMPORAL (fecha de extracci√≥n)
    df['fecha_extraccion'] = pd.to_datetime(df['Fecha Extracci√≥n'])
    df['dia_semana'] = df['fecha_extraccion'].dt.day_name()
    df['es_fin_semana'] = df['dia_semana'].isin(['Saturday', 'Sunday'])
    
    # 8. CRITERIO DE IMPORTANCIA INTELIGENTE
    # Un art√≠culo es importante si cumple m√∫ltiples criterios de calidad period√≠stica
    criterios_importancia = (
        # Contenido sustancial
        (df['longitud_contenido'] > df['longitud_contenido'].quantile(0.7)).astype(int) +
        (df['palabras_contenido'] > df['palabras_contenido'].quantile(0.7)).astype(int) +
        
        # Estructura period√≠stica
        df['titulo_informativo'].astype(int) +
        df['contenido_estructurado'].astype(int) +
        
        # Prestigio del medio
        (df['prestigio_periodico'] == 'alto').astype(int) +
        
        # Relevancia tem√°tica
        (df['relevancia_categoria'] == 'alta').astype(int) +
        
        # Contenido tem√°tico importante
        (df['conteo_politica'] >= 2).astype(int) +
        (df['conteo_economia'] >= 2).astype(int) +
        (df['conteo_internacional'] >= 2).astype(int) +
        
        # Complejidad del contenido
        (df['complejidad_contenido'] > df['complejidad_contenido'].quantile(0.6)).astype(int)
    )
    
    # Art√≠culo importante si cumple 4 o m√°s criterios (m√°s estricto)
    df['es_importante'] = (criterios_importancia >= 4)
    
    # Estad√≠sticas del criterio inteligente
    total_importantes = df['es_importante'].sum()
    porcentaje_importantes = (total_importantes / len(df)) * 100
    
    print(f"\nüìä CRITERIOS INTELIGENTES APLICADOS:")
    print(f"   ‚Ä¢ Contenido sustancial (longitud + palabras)")
    print(f"   ‚Ä¢ Estructura period√≠stica (t√≠tulo + contenido)")
    print(f"   ‚Ä¢ Prestigio del medio (La Vanguardia, Elmundo, Nytimes)")
    print(f"   ‚Ä¢ Relevancia tem√°tica (Internacional, Pol√≠tica, Econom√≠a)")
    print(f"   ‚Ä¢ Contenido tem√°tico (pol√≠tica, econom√≠a, internacional)")
    print(f"   ‚Ä¢ Complejidad del contenido")
    print(f"   ‚Ä¢ Un art√≠culo es importante si cumple 4+ criterios")
    
    print(f"\nüìà RESULTADO DEL CRITERIO INTELIGENTE:")
    print(f"   ‚Ä¢ Art√≠culos importantes: {total_importantes} ({porcentaje_importantes:.1f}%)")
    print(f"   ‚Ä¢ Art√≠culos no importantes: {len(df) - total_importantes} ({100-porcentaje_importantes:.1f}%)")
    
    # An√°lisis por criterios
    print(f"\nüîç AN√ÅLISIS POR CRITERIOS:")
    print(f"   ‚Ä¢ Contenido extenso: {df['longitud_contenido'] > df['longitud_contenido'].quantile(0.7)}")
    print(f"   ‚Ä¢ T√≠tulo informativo: {df['titulo_informativo'].sum()} art√≠culos")
    print(f"   ‚Ä¢ Contenido estructurado: {df['contenido_estructurado'].sum()} art√≠culos")
    print(f"   ‚Ä¢ Peri√≥dicos de alto prestigio: {df['prestigio_periodico'] == 'alto'} art√≠culos")
    print(f"   ‚Ä¢ Categor√≠as de alta relevancia: {df['relevancia_categoria'] == 'alta'} art√≠culos")
    
    return df

def preparar_caracteristicas_avanzadas(df):
    """Preparar caracter√≠sticas avanzadas para ML"""
    print("üîß Preparando caracter√≠sticas avanzadas...")
    
    # TF-IDF del texto combinado
    tfidf = TfidfVectorizer(
        max_features=2000, 
        stop_words=None,
        ngram_range=(1, 2),  # Unigramas y bigramas
        min_df=2,  # M√≠nimo 2 documentos
        max_df=0.95  # M√°ximo 95% de documentos
    )
    texto_combinado = df['T√≠tulo'] + ' ' + df['Resumen'] + ' ' + df['Contenido']
    X_tfidf = tfidf.fit_transform(texto_combinado)
    
    # Caracter√≠sticas num√©ricas avanzadas
    caracteristicas_numericas = [
        'longitud_titulo', 'longitud_resumen', 'longitud_contenido',
        'palabras_titulo', 'palabras_contenido',
        'complejidad_titulo', 'complejidad_contenido',
        'conteo_politica', 'conteo_economia', 'conteo_internacional',
        'conteo_social', 'conteo_tecnologia', 'conteo_cultura'
    ]
    X_numericas = df[caracteristicas_numericas].values
    
    # Codificar variables categ√≥ricas
    le_periodico = LabelEncoder()
    le_categoria = LabelEncoder()
    le_region = LabelEncoder()
    le_prestigio = LabelEncoder()
    le_relevancia = LabelEncoder()
    
    df['periodico_encoded'] = le_periodico.fit_transform(df['Peri√≥dico'])
    df['categoria_encoded'] = le_categoria.fit_transform(df['Categor√≠a'])
    df['region_encoded'] = le_region.fit_transform(df['Regi√≥n'])
    df['prestigio_encoded'] = le_prestigio.fit_transform(df['prestigio_periodico'])
    df['relevancia_encoded'] = le_relevancia.fit_transform(df['relevancia_categoria'])
    
    # Caracter√≠sticas categ√≥ricas codificadas
    X_categoricas = df[[
        'periodico_encoded', 'categoria_encoded', 'region_encoded',
        'prestigio_encoded', 'relevancia_encoded'
    ]].values
    
    # Caracter√≠sticas binarias
    X_binarias = df[[
        'titulo_informativo', 'contenido_estructurado', 'es_fin_semana'
    ]].astype(int).values
    
    # Imputar valores faltantes
    imputer = SimpleImputer(strategy='median')
    X_numericas = imputer.fit_transform(X_numericas)
    
    # Combinar todas las caracter√≠sticas
    X_combined = hstack([X_tfidf, X_numericas, X_categoricas, X_binarias])
    
    # Target
    y = df['es_importante'].astype(int)
    
    print(f"‚úÖ Caracter√≠sticas preparadas: {X_combined.shape}")
    print(f"   ‚Ä¢ TF-IDF: {X_tfidf.shape[1]} caracter√≠sticas")
    print(f"   ‚Ä¢ Num√©ricas: {len(caracteristicas_numericas)} caracter√≠sticas")
    print(f"   ‚Ä¢ Categ√≥ricas: 5 caracter√≠sticas")
    print(f"   ‚Ä¢ Binarias: 3 caracter√≠sticas")
    print(f"   ‚Ä¢ Total: {X_combined.shape[1]} caracter√≠sticas")
    
    return X_combined, y, df

def entrenar_algoritmos_avanzados(X, y):
    """Entrenar algoritmos con caracter√≠sticas avanzadas"""
    print("ü§ñ Entrenando algoritmos avanzados...")
    
    # Dividir datos
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Convertir a denso para algunos algoritmos
    X_train_dense = X_train.toarray()
    X_test_dense = X_test.toarray()
    
    # Escalar caracter√≠sticas
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_dense)
    X_test_scaled = scaler.transform(X_test_dense)
    
    algoritmos = {
        'Regresi√≥n Log√≠stica': LogisticRegression(random_state=42, max_iter=1000),
        'KNN': KNeighborsClassifier(n_neighbors=5),
        'Naive Bayes': MultinomialNB(),
        '√Årbol de Decisi√≥n': DecisionTreeClassifier(random_state=42, max_depth=10),
        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10),
        'SVM': SVC(random_state=42, probability=True, kernel='rbf'),
        'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42, max_depth=10),
        'Ensemble': VotingClassifier([
            ('lr', LogisticRegression(random_state=42, max_iter=1000)),
            ('rf', RandomForestClassifier(random_state=42, n_estimators=50)),
            ('dt', DecisionTreeClassifier(random_state=42, max_depth=10))
        ])
    }
    
    resultados = []
    
    for nombre, modelo in algoritmos.items():
        try:
            print(f"   üîÑ Entrenando {nombre}...")
            
            # Seleccionar datos apropiados
            if nombre == 'Naive Bayes':
                X_train_use = np.abs(X_train_dense)
                X_test_use = np.abs(X_test_dense)
            elif nombre in ['Regresi√≥n Log√≠stica', 'SVM']:
                X_train_use = X_train_scaled
                X_test_use = X_test_scaled
            else:
                X_train_use = X_train_dense
                X_test_use = X_test_dense
            
            # Entrenar
            modelo.fit(X_train_use, y_train)
            
            # Predecir
            y_pred = modelo.predict(X_test_use)
            y_pred_proba = modelo.predict_proba(X_test_use)[:, 1] if hasattr(modelo, 'predict_proba') else y_pred
            
            # M√©tricas
            accuracy = accuracy_score(y_test, y_pred)
            auc = roc_auc_score(y_test, y_pred_proba)
            
            resultados.append({
                'nombre': nombre,
                'accuracy': float(accuracy),
                'auc': float(auc),
                'categoria': 'Clasificaci√≥n'
            })
            
            print(f"      ‚úÖ {nombre}: Accuracy={accuracy:.3f}, AUC={auc:.3f}")
            
        except Exception as e:
            print(f"      ‚ùå Error en {nombre}: {str(e)}")
            resultados.append({
                'nombre': nombre,
                'accuracy': 0.0,
                'auc': 0.5,
                'categoria': 'Clasificaci√≥n',
                'error': str(e)
            })
    
    # Clustering (K-Means)
    try:
        print("   üîÑ Entrenando K-Means...")
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_train_dense)
        
        silhouette = silhouette_score(X_train_dense, clusters)
        
        resultados.append({
            'nombre': 'K-Means',
            'accuracy': 0.0,
            'auc': 0.0,
            'silhouette': float(silhouette),
            'categoria': 'Clustering'
        })
        
        print(f"      ‚úÖ K-Means: Silhouette={silhouette:.3f}")
        
    except Exception as e:
        print(f"      ‚ùå Error en K-Means: {str(e)}")
        resultados.append({
            'nombre': 'K-Means',
            'accuracy': 0.0,
            'auc': 0.0,
            'categoria': 'Clustering',
            'error': str(e)
        })
    
    return resultados, X_test, y_test

def generar_dashboard_inteligente(df, resultados):
    """Generar datos del dashboard con an√°lisis inteligente"""
    print("üìä Generando dashboard inteligente...")
    
    # Ordenar resultados por accuracy
    resultados_ordenados = sorted([r for r in resultados if 'error' not in r], 
                                 key=lambda x: x['accuracy'], reverse=True)
    
    # Agregar ranking y estado
    for i, resultado in enumerate(resultados_ordenados):
        resultado['ranking'] = i + 1
        resultado['id'] = i + 1
        
        if resultado['accuracy'] >= 0.90:
            resultado['estado'] = 'Excelente'
        elif resultado['accuracy'] >= 0.80:
            resultado['estado'] = 'Muy Bueno'
        elif resultado['accuracy'] >= 0.70:
            resultado['estado'] = 'Bueno'
        elif resultado['accuracy'] >= 0.60:
            resultado['estado'] = 'Regular'
        else:
            resultado['estado'] = 'Necesita Mejora'
    
    # Informaci√≥n del dataset
    dataset_info = {
        'total_articulos': len(df),
        'columnas': list(df.columns),
        'periodicos': df['Peri√≥dico'].value_counts().to_dict(),
        'categorias': df['Categor√≠a'].value_counts().to_dict(),
        'regiones': df['Regi√≥n'].value_counts().to_dict(),
        'prestigio_distribucion': df['prestigio_periodico'].value_counts().to_dict(),
        'relevancia_distribucion': df['relevancia_categoria'].value_counts().to_dict()
    }
    
    # Resumen del an√°lisis
    mejor_algoritmo = resultados_ordenados[0] if resultados_ordenados else None
    
    resumen_analisis = {
        'mejor_algoritmo': mejor_algoritmo,
        'metodologia': 'An√°lisis inteligente con criterios period√≠sticos avanzados',
        'criterio_clasificacion': 'M√∫ltiples criterios de calidad period√≠stica: contenido sustancial, estructura period√≠stica, prestigio del medio, relevancia tem√°tica, contenido tem√°tico, complejidad',
        'algoritmos_evaluados': len(resultados),
        'variables_utilizadas': [
            'TF-IDF del texto (t√≠tulos, res√∫menes, contenido)',
            'Longitud y complejidad del contenido',
            'An√°lisis tem√°tico (pol√≠tica, econom√≠a, internacional, etc.)',
            'Prestigio del peri√≥dico',
            'Relevancia de la categor√≠a',
            'Estructura period√≠stica (t√≠tulos informativos, contenido estructurado)',
            'Caracter√≠sticas temporales'
        ]
    }
    
    # Crear estructura completa
    dashboard_data = {
        'dataset_info': dataset_info,
        'resumen_analisis': resumen_analisis,
        'resultados': resultados_ordenados
    }
    
    # Guardar JSON
    with open('dashboard_data_inteligente.json', 'w', encoding='utf-8') as f:
        json.dump(dashboard_data, f, ensure_ascii=False, indent=2)
    
    print("‚úÖ Dashboard inteligente generado: dashboard_data_inteligente.json")
    
    return dashboard_data

def main():
    """Funci√≥n principal"""
    print("üöÄ INICIANDO AN√ÅLISIS INTELIGENTE DEL DATASET")
    print("=" * 70)
    
    # 1. Cargar y analizar dataset
    df = cargar_y_analizar_dataset()
    
    # 2. Crear variables inteligentes
    df = crear_variables_inteligentes(df)
    
    # 3. Preparar caracter√≠sticas avanzadas
    X, y, df = preparar_caracteristicas_avanzadas(df)
    
    # 4. Entrenar algoritmos avanzados
    resultados, X_test, y_test = entrenar_algoritmos_avanzados(X, y)
    
    # 5. Generar dashboard inteligente
    dashboard_data = generar_dashboard_inteligente(df, resultados)
    
    print("\n" + "=" * 70)
    print("üéâ AN√ÅLISIS INTELIGENTE COMPLETADO")
    print("=" * 70)
    
    # Mostrar resumen
    print(f"\nüìä RESUMEN DE RESULTADOS:")
    print(f"   ‚Ä¢ Total de art√≠culos analizados: {len(df)}")
    print(f"   ‚Ä¢ Algoritmos evaluados: {len(resultados)}")
    print(f"   ‚Ä¢ Mejor algoritmo: {dashboard_data['resumen_analisis']['mejor_algoritmo']['nombre']}")
    print(f"   ‚Ä¢ Accuracy del mejor: {dashboard_data['resumen_analisis']['mejor_algoritmo']['accuracy']:.3f}")
    
    print(f"\nüß† CRITERIOS INTELIGENTES APLICADOS:")
    print(f"   ‚Ä¢ Contenido sustancial (longitud + palabras)")
    print(f"   ‚Ä¢ Estructura period√≠stica (t√≠tulos informativos + contenido estructurado)")
    print(f"   ‚Ä¢ Prestigio del medio (La Vanguardia, Elmundo, Nytimes)")
    print(f"   ‚Ä¢ Relevancia tem√°tica (Internacional, Pol√≠tica, Econom√≠a)")
    print(f"   ‚Ä¢ Contenido tem√°tico (an√°lisis de palabras clave)")
    print(f"   ‚Ä¢ Complejidad del contenido")
    print(f"   ‚Ä¢ Un art√≠culo es importante si cumple 4+ criterios")
    
    print(f"\n‚úÖ Archivo generado: dashboard_data_inteligente.json")

if __name__ == "__main__":
    main()
